---
title: 視線入力とコミット
description: "\"宝石とコミット\" 入力モデルの全般的な概要-目または複数の入力を使用します。"
author: sostel
ms.author: sostel
ms.date: 10/31/2019
ms.topic: article
keywords: Mixed Reality、宝石、ビジョン化、相互作用、設計、視線追跡、ヘッドトラッキング
ms.openlocfilehash: c44c1a75e831869a3ed4d12bb6c9e87c478daf56
ms.sourcegitcommit: e65f1463aec3c040a1cd042e61fc2bd156a42ff8
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 05/26/2020
ms.locfileid: "83866892"
---
# <a name="gaze-and-commit"></a>視線入力とコミット

_宝石とコミットメント_は、マウス:_ポイント & クリック_を使用してコンピューターを操作する方法と密接に関連した基本的な入力モデルです。
このページでは、2種類の宝石入力 (ヘッドと視線) とさまざまな種類のコミットアクションを紹介します。 
_宝石とコミットメント_は、間接的な操作による遠くの入力モデルと見なされます。
これは、holographic コンテンツとのやり取りに最適であることを意味します。

Mixed reality ヘッドセットでは、ユーザーのヘッドの位置と向きを使用して、そのヘッド方向ベクターを決定できます。 これは、ユーザーの目と目の間から直接、まっすぐ前を指し示すレーザーと考えることができます。 これはユーザーが目を向けている場所の非常に粗い近似値です。 アプリケーションでは、この射線と仮想または実世界のオブジェクトを交差させ、その位置にカーソルを描画して、現在対象としているものをユーザーに知らせることができます。

また、HoloLens 2 など、一部の mixed reality ヘッドセットには、目を見つめたベクトルを作り出す視線追跡システムが含まれています。 これにより、ユーザーが目を向けている場所のきめ細かい測定値が得られます。 どちらの場合も、宝石はユーザーの意図に対する重要な信号を表します。 システムを使用すると、ユーザーの意図したアクションを解釈および予測することができ、ユーザーの満足度が向上し、パフォーマンスが向上します。

次に示すのは、mixed reality 開発者が頭または目を見つめている場合の利点を示すいくつかの例です。
* アプリでは、シーン内のホログラムを使用して、ユーザーの注意がどこであるかを判断することができます (詳細については、「視線」を参照してください)。
* アプリでは、ユーザーの宝石に基づいてチャネルのジェスチャとコントローラーを押すことができます。これにより、ユーザーは、そのホログラムをシームレスに選択、アクティブ化、グラブ、スクロール、またはその他の方法で操作できます。
* アプリを使用すると、空間マッピングメッシュを使用して、宝石を実際の表面に配置できます。
* アプリでは、ユーザーが重要なオブジェクトの方向を確認し*ていない*ことを知ることができます。これにより、アプリは、そのオブジェクトに対して視覚的な合図やオーディオの手掛かりを与えることができます。

<br>


## <a name="device-support"></a>デバイス サポート

<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
    <tr>
        <td><strong>入力モデル</strong></td>
        <td><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></td>
        <td><a href="https://docs.microsoft.com/hololens/hololens2-hardware"><strong>HoloLens 2</strong></td>
        <td><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></td>
    </tr>
     <tr>
        <td>頭の視線入力とコミット</td>
        <td>✔️ 推奨</td>
        <td>✔️ 推奨 (3 番目の選択肢 -<a href="interaction-fundamentals.md">他のオプションを参照</a>)</td>
        <td>➕ 代替オプション</td>
    </tr>
         <tr>
        <td>目の視線入力とコミット</td>
        <td>❌使用できません</td>
        <td>✔️ 推奨 (3 番目の選択肢 -<a href="interaction-fundamentals.md">他のオプションを参照</a>)</td>
        <td>❌使用できません</td>
    </tr>
</table>


## <a name="gaze"></a>視線入力

### <a name="eye--or-head-gaze"></a>目を通して、またはヘッドを見つめますか。
"視線とコミット" または "ヘッドビジョンとコミット" の入力モデルを使用するかどうかについて、考慮する必要がある事項がいくつかあります。 イマーシブヘッドセットまたは HoloLens (第1世代) 用に開発している場合、選択は簡単です。ヘッドを見つめてコミットすることです。 HoloLens 2 用に開発している場合、選択肢は少し難しくなります。そのため、それぞれに付随する利点と課題を理解することが重要です。
ここでは、ヘッドと視点を見つめたターゲットを比較するために、次の表に示す幅広い pro とをまとめています。 これは完全なものではありません。ここでは、混合現実における視点を見つめたターゲットについてさらに学習することをお勧めします。
* [Hololens 2 の視線追跡](eye-tracking.md): 開発者向けのガイダンスを含む、hololens 2 での新しいアイ tracking 機能の概要。 
* 視線に[よる対話](eye-gaze-interaction.md): 入力として視線追跡を使用することを計画するときの設計上の考慮事項と推奨事項。

<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
   <tr>
        <td><strong>目を見つめたターゲット</strong></td>
        <td><strong>頭の視線入力のターゲット設定</strong></td>
    </tr>
    <tr>
        <td>より!</td>
        <td>かかる</td>
    </tr>
    <tr>
        <td>少ない労力 (ほとんどの場合、本文の移動は必要ありません)</td>
        <td>Fatiguing 可能性があります (例: ネック歪み)</td>
    </tr>
    <tr>
        <td>カーソルは必要ありませんが、微妙なフィードバックをお勧めします。</td>
        <td>カーソルを表示する必要があります</td>
    </tr>
    <tr>
        <td>Smooth 視線は不要 (たとえば、描画には適していません)</td>
        <td>より詳細な制御と明示</td>
    </tr>
    <tr>
        <td>非常に小さいターゲット (小さなボタンや web リンクなど) では難しい</td>
        <td>信頼性! 優れたフォールバック</td>
    </tr>
    <tr>
        <td>...</td>
        <td>...</td>
    </tr>
</table>

見つめとコミットの入力モデルに対して頭を見つめているか、または目を見つめているかを使用するかどうかは、設計上の制約のセットによって異なります。これらの制約については、「視線」、「[コミット](gaze-and-commit-eyes.md)」、および「ヘッドビジョンと[コミット](gaze-and-commit-head.md)」の記事で個別に説明します。

<br>

---

### <a name="cursor"></a>カーソル

:::row:::
    :::column:::
        ヘッドを見つめする場合は、ほとんどのアプリで[カーソル](cursors.md)(またはその他の聴覚/視覚表示) を使用して、ユーザーが対話しようとしている内容に自信を持っている必要があります。 通常、このカーソルを世界中に配置して、頭の中線が1つ目のオブジェクトと交差するようにします。これは、ホログラムや実際の表面です。<br>
        <br>
        目を見つめて、通常はカーソルを表示し*ない*ことをお勧めします。これは、ユーザーにとって煩雑で面倒になるためです。 代わりに、視覚的なターゲットを微妙に強調表示するか、非常に薄い目のカーソルを使用して、ユーザーが何を操作しようとしているかについて自信を持っています。 詳細については、HoloLens 2 での[目に基づく入力の設計ガイダンスを](eye-tracking.md)参照してください。
    :::column-end:::
        :::column:::
       ![宝石を示すビジュアルカーソルの例](images/cursor.jpg)<br>
       *Image: 宝石を示すビジュアルカーソルの例*
    :::column-end:::
:::row-end:::

<br>

---

## <a name="commit"></a>Commit
ターゲットを_見つめ_たさまざまな方法について説明した後、「_宝石とコミットメント_」の_コミット_部分についてもう少し詳しく説明します。
オブジェクトまたは UI 要素をターゲットにした後、ユーザーは2次入力を使用して操作またはクリックできます。 これは、入力モデルのコミットステップと呼ばれます。 

以下のコミット方法がサポートされています。
- エアタップハンドジェスチャ (つまり、手元に手を入れて、インデックスの指とつまみをまとめる)
- _"選択"_ または対象となる音声コマンドの1つを言います。
- [HoloLens Clicker](https://docs.microsoft.com/hololens/hololens1-clicker)で1つのボタンを押す
- Xbox ゲームパッドで [A] ボタンを押す
- Xbox adaptive コントローラーで [A] ボタンを押す

### <a name="gaze-and-air-tap-gesture"></a>宝石とエアタップのジェスチャ
エアタップは、手をまっすぐにしてタップするジェスチャです。 エアタップを実行するには、インデックスの指を準備完了の位置まで上げて、親指でピンチし、インデックスを作成して、リリースまでさかのぼってください。 HoloLens (第1世代) では、無線タップが最も一般的な2番目の入力です。


:::row:::
    :::column:::
       ![準備完了の位置にある指](images/readyandpress-ready.jpg)<br>
       **準備完了の位置にある指**<br>
    :::column-end:::
    :::column:::
       ![指を押しながらタップまたはクリックします。](images/readyandpress-press.jpg)<br>
        **指を押しながらタップまたはクリックします。**<br>
    :::column-end:::
:::row-end:::


無線タップは、HoloLens 2 でも使用できます。 これは、元のバージョンからは緩和されています。 ほとんどすべての種類の pinches は、手が垂直で維持されている限り、サポートされるようになりました。 これによりユーザーは、はるかに簡単にジェスチャを学習したり実行したりできます。 この新しいエアタップでは、古いものが同じ API を使用して置き換えられるため、HoloLens 2 の再コンパイル後に既存のアプリケーションの新しい動作が自動的に行われます。

<br>

---

### <a name="gaze-and-select-voice-command"></a>宝石と "選択" 音声コマンド
音声コマンド処理は、mixed reality の主要な相互作用メソッドの1つです。 システムを制御するための非常に強力な機能を提供します。 ボイス作用モデルには、次のような種類があります。

- クリックを実行する汎用の "Select" コマンド。セカンダリ入力としてコミットします。
- オブジェクトコマンド (例: "Close" または "拡大する") は、アクションを実行し、セカンダリ入力としてコミットします。
- グローバルコマンド (例: "start に移動") では、ターゲットは必要ありません。
- 対話ユーザーインターフェイスまたは Cortana のようなエンティティには、AI 自然言語機能があります。
- カスタム音声コマンド

詳細と、使用可能な音声コマンドの一覧とその使用方法については、「[音声](voice-design.md)コマンドの操作に関するガイダンス」を参照してください。

<br>

---


### <a name="gaze-and-hololens-clicker"></a>宝石と HoloLens Clicker

:::row:::
    :::column:::
        HoloLens Clicker は、HoloLens 専用に構築された最初の周辺機器です。 これは HoloLens (第1世代) Development Edition に含まれています。 HoloLens Clicker を使用すると、ユーザーは最小限の手でクリックし、2番目の入力としてコミットできます。 HoloLens Clicker は、Bluetooth 低エネルギー (BTLE) を使用して HoloLens (第1世代) または HoloLens 2 に接続します。<br>
        <br>
        [デバイスをペアリングするための詳細情報と手順](hardware-accessories.md#pairing-bluetooth-accessories)<br>
        <br>
        *イメージ: HoloLens Clicker*
    :::column-end:::
        :::column:::
       ![HoloLens クリッカー](images/hololens-clicker-500px.jpg)<br>
    :::column-end:::
:::row-end:::

<br>

---


### <a name="gaze-and-xbox-wireless-controller"></a>宝石と Xbox ワイヤレスコントローラー

:::row:::
    :::column:::
        Xbox ワイヤレスコントローラーは、"A" ボタンを使用して、2番目の入力としてクリックを実行します。 デバイスは、システムの移動と制御に役立つ既定のアクションセットにマップされます。 コントローラーをカスタマイズする場合は、Xbox アクセサリアプリケーションを使用して、Xbox ワイヤレスコントローラーを構成します。<br>
        <br>
        [Xbox コントローラーを PC とペアリングする方法](hardware-accessories.md#pairing-bluetooth-accessories)<br>
        <br>
        *イメージ: Xbox ワイヤレスコントローラー*
    :::column-end:::
        :::column:::
       ![Xbox ワイヤレス コントローラー](images/xboxcontroller.jpg)<br>
    :::column-end:::
:::row-end:::



<br>

---


### <a name="gaze-and-xbox-adaptive-controller"></a>宝石と Xbox Adaptive Controller
多くの場合、モビリティが制限されたゲーマーのニーズを満たすように設計されています。 Xbox Adaptive Controller はデバイス用の統合されたハブであり、混合の現実によりアクセスしやすくなります。

Xbox Adaptive Controller は、"A" ボタンを使用して、2番目の入力としてクリックを実行します。 デバイスは、システムの移動と制御に役立つ既定のアクションセットにマップされます。 コントローラーをカスタマイズする場合は、Xbox アクセサリアプリケーションを使用して Xbox Adaptive コントローラーを構成します。

![Xbox Adaptive Controller](images/xbox-adaptive-controller-devices.jpg)<br>
*Xbox Adaptive Controller*

スイッチ、ボタン、マウント、ジョイスティックなどの外部デバイスを接続して、独自のカスタムコントローラーエクスペリエンスを作成します。 ボタン、サムスティック、およびトリガーの入力は、3.5 mm ジャックと USB ポートを介して接続されている補助デバイスで制御されます。

![Xbox Adaptive Controller のポート](images/xbox-adaptive-controller-ports.jpg)<br>
*Xbox Adaptive Controller のポート*

[デバイスをペアリングする手順](hardware-accessories.md#pairing-bluetooth-accessories)

<a href=https://www.xbox.com/xbox-one/accessories/controllers/xbox-adaptive-controller>Xbox のサイトで参照できる詳細情報</a>

<br>

---

## <a name="composite-gestures"></a>複合ジェスチャ

### <a name="air-tap"></a>エアタップ
エアタップジェスチャ (およびその他のジェスチャ) は、特定の tap にのみ反応します。 メニューやつかみなど、他のタップを検出するには、前の「2つの主要なコンポーネントのジェスチャ」セクションで説明されている下位レベルの相互作用をアプリケーションで直接使用する必要があります。

### <a name="tap-and-hold"></a>タップ アンド ホールド
ホールドは、単にエアタップの下向きの指の位置を保持することです。 エアタップとホールディングを組み合わせることにより、さまざまな複雑な "クリックアンドドラッグ" の相互作用が可能になります。たとえば、オブジェクトをアクティブ化する代わりにオブジェクトを選択したり、コンテキストメニューを表示するなどの二次的な相互作用を待機したりすることができます。
ただし、このジェスチャの設計時には注意が必要です。これは、ユーザーはジェスチャを長く続けると途中で手の姿勢を緩める傾向があるためです。

### <a name="manipulation"></a>操作
操作ジェスチャを使用すると、ホログラムがユーザーの手の動きに1:1 を反応させる場合に、ホログラムの移動、サイズ変更、または回転を行うことができます。 このような 1 対 1 の動きの 1 つの用途は、ユーザーが世界中で絵を描いたりペイントしたりできるようにすることです。
操作のジェスチャの最初のターゲット設定は、視線入力またはポインティングによって行う必要があります。 タップとホールドが開始されると、オブジェクトの操作は手動で処理され、ユーザーが操作中に見えなくなるのを解放します。

### <a name="navigation"></a>［ナビゲーション］
ナビゲーションのジェスチャは仮想ジョイスティックのように動作し、リング メニューなどの UI ウィジェット内で移動するために使用できます。 タップ アンド ホールドでジェスチャを始めてから、最初に押したところを中心に、正規化された 3D 立方体の中で手を動かします。 開始点 0 で、-1 から 1 までの値から X、Y、または Z 軸に沿って手を動かすことができます。
ナビゲーションを使用すると、マウスの中央ボタンをクリックしてからマウスを上下に移動して 2 次元の UI をスクロールするのと同様に、速度ベースの連続したスクロールやズームのジェスチャを作成できます。

Rails を使用したナビゲーションとは、特定の軸で特定のしきい値に達するまで移動を認識する機能を指します。 これは、開発者によってアプリケーションで複数の軸での移動が有効になっている場合にのみ役立ちます。たとえば、アプリケーションが X 軸と Y 軸にわたるナビゲーションジェスチャを認識するように構成されていても、X 軸に rails が指定されている場合などです。 この場合、システムは x 軸上の架空のレール (ガイド) 内にある限り、X 軸を越えた手の移動を認識します。これは、Y 軸でも手動で移動した場合に発生します。

2D のアプリ内では、ユーザーは、アプリ内でスクロール、ズーム、またはドラッグするために、垂直方向のナビゲーション ジェスチャを使用できます。 これは、同じ種類のタッチ ジェスチャをシミュレートするために、アプリに仮想の指でのタッチを挿入します。 ユーザーは、アプリケーションの上部にあるバーのツールを切り替えることによって実行するアクションを選択できます。これを行うには、ボタンを選択するか、[スクロール/ドラッグ/ズーム> ツールを <] をクリックします。

[複合ジェスチャに関する詳細情報](gaze-and-commit.md#composite-gestures)

## <a name="gesture-recognizers"></a>ジェスチャ認識エンジン

ジェスチャ認識を使用する利点の1つは、現在ターゲットとなっているホログラムが受け入れるジェスチャに対してのみジェスチャ認識エンジンを構成できることです。 プラットフォームは、サポートされている特定のジェスチャを区別するために、必要に応じてあいまいさを解消します。 このようにして、エアタップをサポートしているホログラムは、プレスとリリースの間に任意の時間を受け入れることができます。一方、タップとホールドの両方をサポートするホログラムは、ホールド時間のしきい値を超えたときにタップを保留に昇格させることができます。

## <a name="hand-recognition"></a>手の認識
HoloLens は、デバイスで確認できる片手または両手の位置を追跡することで、手のジェスチャを認識します。 HoloLens は、手が準備完了状態 (手の甲を自分に向けて人差し指を立てる) または押された状態 (手の甲を自分に向けて人差し指を曲げる) のいずれかの状態のときに、手を認識します。 他の人に手を付けると、HoloLens はそれらを無視します。
HoloLens が検出した各ハンドでは、向きと押されていない状態でその位置にアクセスできます。 手がジェスチャ フレームの端に近づくと、方向ベクトルも表示されます。これをユーザーに示すことで、ユーザーは、どのように手を動かせば、HoloLens が認識できる位置に戻せるかを知ることができます。

## <a name="gesture-frame"></a>ジェスチャ フレーム
HoloLens でのジェスチャでは、ジェスチャが検出されたカメラが適切に見える範囲で、ウェストとショルダーの間にジェスチャを使用する必要があります。 ユーザーは、正常に動作していて快適にするために、この認識の領域でトレーニングを受ける必要があります。 多くのユーザーは、最初に、ジェスチャフレームが HoloLens を通じて表示されている必要があります。また、対話するために、そのアームをすぐに保持しておく必要があります。 HoloLens Clicker を使用する場合、ジェスチャフレーム内にハンドを配置する必要はありません。

特に連続したジェスチャの場合、ユーザーが holographic オブジェクトを移動するときにジェスチャの途中でハンドを動かしたときに、意図した結果が失われる危険性があります。

考慮すべきことが 3 つあります。

- ジェスチャフレームの存在とおおよその境界に関するユーザー教育。 これは、HoloLens セットアップ中に学習されます。

- アプリケーション内のジェスチャが近づいているか、ジェスチャフレームの境界が失われた場合に、望ましくない結果につながることをユーザーに通知します。 調査には、このような通知システムの主要な品質が示されています。 HoloLens シェルは、この種類の通知の良い例を提供しています。つまり、中央カーソルで、境界の交差が行われている方向を示しています。

- ジェスチャ フレームの境界を越えることによる影響は、最小限に抑える必要があります。 一般に、これは、ジェスチャの結果を、逆順ではなく境界で停止する必要があることを意味します。 たとえば、ユーザーがある程度の holographic オブジェクトを部屋に移動する場合、ジェスチャフレームが侵害されたときに移動が停止し、開始位置には返されません。 ユーザーにはいくつかのフラストレーションが生じる可能性がありますが、境界をよりよく理解しておくことが必要であり、すべての目的のアクションを毎回再起動する必要はありません。



## <a name="see-also"></a>関連項目
* [視線ベースの操作](eye-gaze-interaction.md)
* [HoloLens 2 上の視線追跡](eye-tracking.md)
* [視線入力とドウェル](gaze-and-dwell.md)
* [手 - 直接操作](direct-manipulation.md)
* [手 - ジェスチャ](gaze-and-commit.md#composite-gestures)
* [手 - ポイントとコミット](point-and-commit.md)
* [本能的な操作](interaction-fundamentals.md)
* [音声入力](voice-input.md)

